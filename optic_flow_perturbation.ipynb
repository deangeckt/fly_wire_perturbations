{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1df8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madis\\Documents\\Perturbations\\fly_wire_perturbations\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flyvis_cell_type_pert import FlyvisCellTypePert, PerturbationType\n",
    "from flyvis.datasets.sintel import MultiTaskSintel\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "import datamate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826b9ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Patched datamate.directory._write_h5\n",
      "Importing flyvis...\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/flyvis_data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"FLYVIS_ROOT_DIR\"] = str(data_path)\n",
    "\n",
    "def fixed_write_h5(path, val):\n",
    "    \"\"\"\n",
    "    A Windows-safe replacement that skips the 'read-before-write' check.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with h5py.File(path, mode=\"w\", libver=\"latest\") as f:\n",
    "        f.create_dataset(\"data\", data=val)\n",
    "\n",
    "datamate.io._write_h5 = fixed_write_h5\n",
    "if hasattr(datamate.directory, \"_write_h5\"):\n",
    "    datamate.directory._write_h5 = fixed_write_h5\n",
    "    print(\" -> Patched datamate.directory._write_h5\")\n",
    "else:\n",
    "    print(\" -> Warning: Could not find _write_h5 in directory module\")\n",
    "\n",
    "print(\"Importing flyvis...\")\n",
    "from flyvis import NetworkView\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 3,
>>>>>>> 02bcba78fdf67194c7fee42a379f1e4e1babffe4
   "id": "0186b5ad",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-07 19:26:47] sintel_utils:331 Found Sintel at c:\\Users\\dean\\Documents\\dev\\fly_winter_school\\fly_wire_perturbations\\.venv\\Lib\\site-packages\\flyvis\\data\\SintelDataSet\n"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Sintel dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-07 17:02:55] sintel_utils:324 Downloading Sintel dataset.\n",
      "100%|██████████| 5.24G/5.24G [1:18:06<00:00, 1.20MB/s]\n",
      "[2026-01-07 18:21:02] sintel_utils:327 Extracting Sintel dataset.\n",
      "Rendering: 100%|██████████| 23/23 [01:49<00:00,  4.78s/it]\n"
>>>>>>> 02bcba78fdf67194c7fee42a379f1e4e1babffe4
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Initializing Sintel dataset...\n",
=======
>>>>>>> 02bcba78fdf67194c7fee42a379f1e4e1babffe4
      "Dataset initialized with 23 sequences\n",
      "First 5 sequences: ['sequence_00_alley_1_split_00', 'sequence_01_alley_2_split_00', 'sequence_02_ambush_2_split_00', 'sequence_03_ambush_4_split_00', 'sequence_04_ambush_5_split_00']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>original_index</th>\n",
       "      <th>name</th>\n",
       "      <th>original_n_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sequence_00_alley_1_split_00</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sequence_01_alley_2_split_00</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sequence_02_ambush_2_split_00</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>sequence_03_ambush_4_split_00</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>sequence_04_ambush_5_split_00</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  original_index                           name  original_n_frames\n",
       "0      0               0   sequence_00_alley_1_split_00                 50\n",
       "1      1               1   sequence_01_alley_2_split_00                 50\n",
       "2      2               2  sequence_02_ambush_2_split_00                 21\n",
       "3      3               3  sequence_03_ambush_4_split_00                 33\n",
       "4      4               4  sequence_04_ambush_5_split_00                 50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Initializing Sintel dataset...\")\n",
    "\n",
    "dataset = MultiTaskSintel(\n",
    "    tasks=[\"flow\"],\n",
    "    boxfilter=dict(extent=15, kernel_size=13),\n",
    "    vertical_splits=1,  # Use 1 for faster testing, 3 for full dataset\n",
    "    n_frames=19,\n",
    "    dt=1/50,  # Temporal resolution\n",
    "    augment=False,  # Set to False for evaluation\n",
    "    resampling=True,\n",
    "    interpolate=True,\n",
    "    all_frames=False,\n",
    "    random_temporal_crop=False,\n",
    ")\n",
    "\n",
    "print(f\"Dataset initialized with {len(dataset)} sequences\")\n",
    "if hasattr(dataset, 'arg_df'):\n",
    "    print(f\"First 5 sequences: {dataset.arg_df['name'].tolist()[:5]}\")\n",
    "    display(dataset.arg_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001e580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting first sample...\n",
      "Sample keys: dict_keys(['lum', 'flow'])\n",
      "Luminance shape: torch.Size([40, 1, 721])\n",
      "Flow shape: torch.Size([40, 2, 721])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dean\\Documents\\dev\\fly_winter_school\\fly_wire_perturbations\\.venv\\Lib\\site-packages\\torch\\utils\\_device.py:103: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\python_variable_indexing.cpp:351.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# %% Inspect a Single Sample\n",
    "print(\"\\nInspecting first sample...\")\n",
    "sample = dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Luminance shape: {sample['lum'].shape}\")\n",
    "print(f\"Flow shape: {sample['flow'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ce7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  %% Load Network and Decoder\n",
    "print(\"\\nLoading network...\")\n",
    "src_folder = data_path / \"results/flow/0000/000\"\n",
    "network_view = NetworkView(src_folder)\n",
    "network = network_view.init_network()\n",
    "\n",
    "print(\"Loading decoder...\")\n",
    "decoder = network_view.init_decoder()[\"flow\"]\n",
    "decoder.eval()\n",
    "\n",
    "print(f\"Network initialized successfully\")\n",
    "print(f\"Number of network parameters: {sum(p.numel() for p in network.parameters())}\")\n",
    "print(f\"Number of decoder parameters: {sum(p.numel() for p in decoder.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b27210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Test Single Sequence Prediction\n",
    "print(\"\\nTesting prediction on first sequence...\")\n",
    "data = dataset[0]\n",
    "lum = data[\"lum\"]\n",
    "flow = data[\"flow\"]\n",
    "\n",
    "# Simulate network response\n",
    "stationary_state = network.fade_in_state(1.0, dataset.dt, lum[[0]])\n",
    "responses = network.simulate(lum[None], dataset.dt, initial_state=stationary_state)\n",
    "\n",
    "# Decode flow from neural responses\n",
    "y_pred = decoder(responses)\n",
    "\n",
    "# Compute EPE for this sequence\n",
    "epe = torch.sqrt(((y_pred - flow) ** 2).sum(dim=1))\n",
    "\n",
    "print(f\"Prediction shape: {y_pred.shape}\")\n",
    "print(f\"Ground truth shape: {flow.shape}\")\n",
    "print(f\"EPE shape: {epe.shape}\")\n",
    "print(f\"Mean EPE: {epe.mean().item():.4f} pixels\")\n",
    "print(f\"Median EPE: {epe.median().item():.4f} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4aa4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Full Evaluation Function\n",
    "\n",
    "# %% Define Evaluation Function\n",
    "def evaluate_network(network, decoder, dataset, output_file=None):\n",
    "    \"\"\"\n",
    "    Evaluate network on entire Sintel dataset\n",
    "    \"\"\"\n",
    "    print('Generating Sintel optic flow responses...')\n",
    "    \n",
    "    all_pred_flow = []\n",
    "    all_true_flow = []\n",
    "    all_epe = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processing sequence {i+1}/{len(dataset)}...\")\n",
    "        \n",
    "        data = dataset[i]\n",
    "        lum = data[\"lum\"]\n",
    "        flow = data[\"flow\"]\n",
    "        \n",
    "        # Simulate network response\n",
    "        stationary_state = network.fade_in_state(1.0, dataset.dt, lum[[0]])\n",
    "        responses = network.simulate(lum[None], dataset.dt, initial_state=stationary_state)\n",
    "        \n",
    "        # Decode flow from neural responses\n",
    "        y_pred = decoder(responses)\n",
    "        \n",
    "        # Compute EPE for this sequence\n",
    "        epe = torch.sqrt(((y_pred - flow) ** 2).sum(dim=1))\n",
    "        \n",
    "        all_pred_flow.append(y_pred.detach().cpu())\n",
    "        all_true_flow.append(flow.cpu() if hasattr(flow, 'cpu') else flow)\n",
    "        all_epe.append(epe.detach().cpu())\n",
    "    \n",
    "    print('Evaluating performance...')\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    all_epe_tensor = torch.cat(all_epe, dim=0)\n",
    "    \n",
    "    # Compute overall statistics\n",
    "    results = []\n",
    "    \n",
    "    results.append({\n",
    "        'sequence': 'overall',\n",
    "        'n_sequences': len(dataset),\n",
    "        'mean_epe': float(all_epe_tensor.mean()),\n",
    "        'median_epe': float(all_epe_tensor.median()),\n",
    "        'std_epe': float(all_epe_tensor.std()),\n",
    "        'epe_pixel_1': float((all_epe_tensor < 1).float().mean()),\n",
    "        'epe_pixel_3': float((all_epe_tensor < 3).float().mean()),\n",
    "        'epe_pixel_5': float((all_epe_tensor < 5).float().mean()),\n",
    "    })\n",
    "    \n",
    "    # Per-sequence statistics\n",
    "    for i, epe in enumerate(all_epe):\n",
    "        results.append({\n",
    "            'sequence': f'seq_{i:03d}',\n",
    "            'sequence_name': dataset.arg_df.iloc[i]['name'] if hasattr(dataset, 'arg_df') else f'seq_{i}',\n",
    "            'mean_epe': float(epe.mean()),\n",
    "            'median_epe': float(epe.median()),\n",
    "            'std_epe': float(epe.std()),\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if output_file:\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    # Print summary\n",
    "    overall = results_df[results_df['sequence'] == 'overall'].iloc[0]\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Mean EPE: {overall['mean_epe']:.4f} pixels\")\n",
    "    print(f\"  Median EPE: {overall['median_epe']:.4f} pixels\")\n",
    "    print(f\"  % pixels with EPE < 3px: {overall['epe_pixel_3']*100:.2f}%\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "08ecb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on first 3 sequences only for quick test\n",
    "class SubsetDataset:\n",
    "    def __init__(self, dataset, n_samples=3):\n",
    "        self.dataset = dataset\n",
    "        self.n_samples = min(n_samples, len(dataset))\n",
    "        self.dt = dataset.dt\n",
    "        self.arg_df = dataset.arg_df.iloc[:self.n_samples] if hasattr(dataset, 'arg_df') else None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "subset_dataset = SubsetDataset(test_dataset, n_samples=3)\n",
    "results_original = evaluate_network(\n",
    "    network, \n",
    "    decoder, \n",
    "    subset_dataset,\n",
    "    output_file=\"data/flyvis_data/perf/sintel_original_quick_test.csv\"\n",
    ")\n",
    "\n",
    "display(results_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 02bcba78fdf67194c7fee42a379f1e4e1babffe4
   "id": "511034ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load Connectivity and Set Up Perturbation\n",
    "conn_df = pd.read_csv('data/flyvis_data/flyvis_cell_type_connectivity.csv')\n",
    "print(f\"Loaded connectivity with {len(conn_df)} connections\")\n",
    "print(f\"\\nColumns: {conn_df.columns.tolist()}\")\n",
    "\n",
    "# %% Define Perturbation\n",
    "pert = FlyvisCellTypePert()\n",
    "pairs_to_perturb = [('L4', 'L4')]\n",
    "\n",
    "pert.perturb(conn_df, PerturbationType.PAIR_WISE, pairs=pairs_to_perturb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
