{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1df8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madis\\Documents\\Perturbations\\fly_wire_perturbations\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Patched datamate.directory._write_h5\n",
      "Importing flyvis...\n"
     ]
    }
   ],
   "source": [
    "from flyvis_cell_type_pert import FlyvisCellTypePert, PerturbationType\n",
    "from flyvis.datasets.sintel import MultiTaskSintel\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "import datamate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from optic_flow import SintelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826b9ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Patched datamate.directory._write_h5\n",
      "Importing flyvis...\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"data/flyvis_data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"FLYVIS_ROOT_DIR\"] = str(data_path)\n",
    "\n",
    "def fixed_write_h5(path, val):\n",
    "    \"\"\"\n",
    "    A Windows-safe replacement that skips the 'read-before-write' check.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with h5py.File(path, mode=\"w\", libver=\"latest\") as f:\n",
    "        f.create_dataset(\"data\", data=val)\n",
    "\n",
    "datamate.io._write_h5 = fixed_write_h5\n",
    "if hasattr(datamate.directory, \"_write_h5\"):\n",
    "    datamate.directory._write_h5 = fixed_write_h5\n",
    "    print(\" -> Patched datamate.directory._write_h5\")\n",
    "else:\n",
    "    print(\" -> Warning: Could not find _write_h5 in directory module\")\n",
    "\n",
    "print(\"Importing flyvis...\")\n",
    "from flyvis import NetworkView\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0186b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-07 23:01:00] sintel_utils:331 Found Sintel at c:\\Users\\madis\\Documents\\Perturbations\\fly_wire_perturbations\\.venv\\Lib\\site-packages\\flyvis\\data\\SintelDataSet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Sintel dataset...\n",
      "Dataset initialized with 23 sequences\n",
      "First 5 sequences: ['sequence_00_alley_1_split_00', 'sequence_01_alley_2_split_00', 'sequence_02_ambush_2_split_00', 'sequence_03_ambush_4_split_00', 'sequence_04_ambush_5_split_00']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>original_index</th>\n",
       "      <th>name</th>\n",
       "      <th>original_n_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sequence_00_alley_1_split_00</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sequence_01_alley_2_split_00</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sequence_02_ambush_2_split_00</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>sequence_03_ambush_4_split_00</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>sequence_04_ambush_5_split_00</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  original_index                           name  original_n_frames\n",
       "0      0               0   sequence_00_alley_1_split_00                 50\n",
       "1      1               1   sequence_01_alley_2_split_00                 50\n",
       "2      2               2  sequence_02_ambush_2_split_00                 21\n",
       "3      3               3  sequence_03_ambush_4_split_00                 33\n",
       "4      4               4  sequence_04_ambush_5_split_00                 50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Initializing Sintel dataset...\")\n",
    "\n",
    "dataset = MultiTaskSintel(\n",
    "    tasks=[\"flow\"],\n",
    "    boxfilter=dict(extent=15, kernel_size=13),\n",
    "    vertical_splits=1,  # Use 1 for faster testing, 3 for full dataset\n",
    "    n_frames=19,\n",
    "    dt=1/50,  # Temporal resolution\n",
    "    augment=False,  # Set to False for evaluation\n",
    "    resampling=True,\n",
    "    interpolate=True,\n",
    "    all_frames=False,\n",
    "    random_temporal_crop=False,\n",
    ")\n",
    "\n",
    "print(f\"Dataset initialized with {len(dataset)} sequences\")\n",
    "if hasattr(dataset, 'arg_df'):\n",
    "    print(f\"First 5 sequences: {dataset.arg_df['name'].tolist()[:5]}\")\n",
    "    display(dataset.arg_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Inspect a Single Sample\n",
    "print(\"\\nInspecting first sample...\")\n",
    "sample = dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Luminance shape: {sample['lum'].shape}\")\n",
    "print(f\"Flow shape: {sample['flow'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ce7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  %% Load Network and Decoder\n",
    "print(\"\\nLoading network...\")\n",
    "src_folder = data_path / \"results/flow/0000/000\"\n",
    "network_view = NetworkView(src_folder)\n",
    "network = network_view.init_network()\n",
    "\n",
    "print(\"Loading decoder...\")\n",
    "decoder = network_view.init_decoder()[\"flow\"]\n",
    "decoder.eval()\n",
    "\n",
    "print(f\"Network initialized successfully\")\n",
    "print(f\"Number of network parameters: {sum(p.numel() for p in network.parameters())}\")\n",
    "print(f\"Number of decoder parameters: {sum(p.numel() for p in decoder.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b27210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Test Single Sequence Prediction\n",
    "print(\"\\nTesting prediction on first sequence...\")\n",
    "data = dataset[0]\n",
    "lum = data[\"lum\"]\n",
    "flow = data[\"flow\"]\n",
    "\n",
    "# Simulate network response\n",
    "stationary_state = network.fade_in_state(1.0, dataset.dt, lum[[0]])\n",
    "responses = network.simulate(lum[None], dataset.dt, initial_state=stationary_state)\n",
    "\n",
    "# Decode flow from neural responses\n",
    "y_pred = decoder(responses)\n",
    "\n",
    "# Compute EPE for this sequence\n",
    "epe = torch.sqrt(((y_pred - flow) ** 2).sum(dim=1))\n",
    "\n",
    "print(f\"Prediction shape: {y_pred.shape}\")\n",
    "print(f\"Ground truth shape: {flow.shape}\")\n",
    "print(f\"EPE shape: {epe.shape}\")\n",
    "print(f\"Mean EPE: {epe.mean().item():.4f} pixels\")\n",
    "print(f\"Median EPE: {epe.median().item():.4f} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4aa4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(network, decoder, dataset, output_file=None):\n",
    "    \"\"\"\n",
    "    Evaluate network on entire Sintel dataset\n",
    "    \"\"\"\n",
    "    print('Generating Sintel optic flow responses...')\n",
    "    \n",
    "    all_pred_flow = []\n",
    "    all_true_flow = []\n",
    "    all_epe = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processing sequence {i+1}/{len(dataset)}...\")\n",
    "        \n",
    "        data = dataset[i]\n",
    "        lum = data[\"lum\"]\n",
    "        flow = data[\"flow\"]\n",
    "        \n",
    "        # Simulate network response\n",
    "        stationary_state = network.fade_in_state(1.0, dataset.dt, lum[[0]])\n",
    "        responses = network.simulate(lum[None], dataset.dt, initial_state=stationary_state)\n",
    "        \n",
    "        # Decode flow from neural responses\n",
    "        y_pred = decoder(responses)\n",
    "        \n",
    "        # Compute EPE for this sequence\n",
    "        epe = torch.sqrt(((y_pred - flow) ** 2).sum(dim=1))\n",
    "        \n",
    "        all_pred_flow.append(y_pred.detach().cpu())\n",
    "        all_true_flow.append(flow.cpu() if hasattr(flow, 'cpu') else flow)\n",
    "        all_epe.append(epe.detach().cpu())\n",
    "    \n",
    "    print('Evaluating performance...')\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    all_epe_tensor = torch.cat(all_epe, dim=0)\n",
    "    \n",
    "    # Compute overall statistics\n",
    "    results = []\n",
    "    \n",
    "    results.append({\n",
    "        'sequence': 'overall',\n",
    "        'n_sequences': len(dataset),\n",
    "        'mean_epe': float(all_epe_tensor.mean()),\n",
    "        'median_epe': float(all_epe_tensor.median()),\n",
    "        'std_epe': float(all_epe_tensor.std()),\n",
    "        'epe_pixel_1': float((all_epe_tensor < 1).float().mean()),\n",
    "        'epe_pixel_3': float((all_epe_tensor < 3).float().mean()),\n",
    "        'epe_pixel_5': float((all_epe_tensor < 5).float().mean()),\n",
    "    })\n",
    "    \n",
    "    # Per-sequence statistics\n",
    "    for i, epe in enumerate(all_epe):\n",
    "        results.append({\n",
    "            'sequence': f'seq_{i:03d}',\n",
    "            'sequence_name': dataset.arg_df.iloc[i]['name'] if hasattr(dataset, 'arg_df') else f'seq_{i}',\n",
    "            'mean_epe': float(epe.mean()),\n",
    "            'median_epe': float(epe.median()),\n",
    "            'std_epe': float(epe.std()),\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if output_file:\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    # Print summary\n",
    "    overall = results_df[results_df['sequence'] == 'overall'].iloc[0]\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Mean EPE: {overall['mean_epe']:.4f} pixels\")\n",
    "    print(f\"  Median EPE: {overall['median_epe']:.4f} pixels\")\n",
    "    print(f\"  % pixels with EPE < 3px: {overall['epe_pixel_3']*100:.2f}%\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate_network(network, decoder, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_df = pd.read_csv(f'{data_path}/flyvis_cell_type_connectivity.csv')\n",
    "\n",
    "result_csv = pd.DataFrame()\n",
    "\n",
    "for src, tar in tqdm(cell_type_df[['source_type', 'target_type']].values):\n",
    "    print(f\"Running perturbation: {src} -> {tar}\")\n",
    "\n",
    "    pert = FlyvisCellTypePert()\n",
    "    pert.perturb(cell_type_df, PerturbationType.PAIR_WISE, pairs=[(src, tar)])\n",
    "\n",
    "    wrapper = SintelWrapper(dataset, pert=pert, pert_folder_name=f'{src}_{tar}_perturbation')\n",
    "    result = wrapper.run()  \n",
    "    \n",
    "    # Add the source-target info\n",
    "    result['source_target_pair'] = f'{src}_{tar}'\n",
    "    result['source_type'] = src\n",
    "    result['target_type'] = tar\n",
    "    \n",
    "    result_csv = pd.concat([result_csv, result], ignore_index=True)\n",
    "\n",
    "result_csv.to_csv(f'data/flyvis_data/optic_flow/pairwise_perturbation_sintel_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e5534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-07 23:01:10] network_view:122 Initialized network view at C:\\Users\\madis\\Documents\\Perturbations\\fly_wire_perturbations\\data\\flyvis_data\\results\\flow\\0000\\000_Tm9_T5a_perturbation\n",
      "[2026-01-07 23:01:10] logging_utils:23 epe not in C:\\Users\\madis\\Documents\\Perturbations\\fly_wire_perturbations\\data\\flyvis_data\\results\\flow\\0000\\000_Tm9_T5a_perturbation\\validation, but 'loss' is. Falling back to 'loss'. You can rerun the ensemble validation to make appropriate recordings of the losses.\n",
      "[2026-01-07 23:01:13] network:222 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.\n",
      "[2026-01-07 23:01:13] chkpt_utils:36 Recovered network state.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sintel optic flow simulation...\n",
      "Applying perturbation to network in memory...\n",
      "Overwriting disk checkpoints with perturbed weights...\n",
      " -> Updated: data\\flyvis_data\\results\\flow\\0000\\000_Tm9_T5a_perturbation\\best_chkpt\n",
      " -> Updated: data\\flyvis_data\\results\\flow\\0000\\000_Tm9_T5a_perturbation\\chkpts\\chkpt_00000\n",
      "Clearing caches...\n",
      " -> Removed __cache__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-07 23:01:14] network_view:122 Initialized network view at C:\\Users\\madis\\Documents\\Perturbations\\fly_wire_perturbations\\data\\flyvis_data\\results\\flow\\0000\\000_Tm9_T5a_perturbation\n",
      "[2026-01-07 23:01:17] network:222 Initialized network with NumberOfParams(free=734, fixed=2959) parameters.\n",
      "[2026-01-07 23:01:17] chkpt_utils:36 Recovered network state.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing decoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-07 23:01:19] decoder:282 Initialized decoder with NumberOfParams(free=7427, fixed=0) parameters.\n",
      "[2026-01-07 23:01:19] decoder:283 DecoderGAVP(\n",
      "  (base): Sequential(\n",
      "    (0): Conv2dHexSpace(34, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Softplus(beta=1.0, threshold=20.0)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv2dHexSpace(8, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (head): Sequential()\n",
      ")\n",
      "[2026-01-07 23:01:19] chkpt_utils:65 Recovered flow decoder state.\n",
      "c:\\Users\\madis\\Documents\\Perturbations\\fly_wire_perturbations\\.venv\\Lib\\site-packages\\torch\\utils\\_device.py:103: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\python_variable_indexing.cpp:351.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Sintel optic flow responses...\n",
      "Evaluating performance...\n",
      "\n",
      "Results saved to \n"
     ]
    }
   ],
   "source": [
    "cell_type_df = pd.read_csv(f'{data_path}/flyvis_cell_type_connectivity.csv')\n",
    "\n",
    "result_csv = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "src = 'Tm9'\n",
    "tar = 'T5a'\n",
    "pert = FlyvisCellTypePert()\n",
    "pert.perturb(cell_type_df, PerturbationType.PAIR_WISE, pairs=[(src, tar)])\n",
    "\n",
    "wrapper = SintelWrapper(dataset, pert=pert, pert_folder_name=f'{src}_{tar}_perturbation')\n",
    "result = wrapper.run()  \n",
    "    \n",
    "    # Add the source-target info\n",
    "result['source_target_pair'] = f'{src}_{tar}'\n",
    "result['source_type'] = src\n",
    "result['target_type'] = tar\n",
    "    \n",
    "result_csv = pd.concat([result_csv, result], ignore_index=True)\n",
    "\n",
    "result_csv.to_csv(f'data/flyvis_data/optic_flow/perturbation_results_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
